<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>MNIST + Tensorflow â€¢ MNIST</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">MNIST</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/stillmatic/MNIST">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>MNIST + Tensorflow</h1>
                        <h4 class="author">Chris Hua</h4>
            
            <h4 class="date">2017-04-26</h4>
          </div>

    
    
<div class="contents">
<div id="loading" class="section level2">
<h2 class="hasAnchor">
<a href="#loading" class="anchor"></a>Loading</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MNIST)
<span class="kw">library</span>(tensorflow)
<span class="kw">library</span>(dplyr)</code></pre></div>
<pre><code>## 
## Attaching package: 'dplyr'</code></pre>
<pre><code>## The following objects are masked from 'package:stats':
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union</code></pre>
<p>Load that data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">datasets &lt;-<span class="st"> </span>tf$contrib$learn$datasets
mnist &lt;-<span class="st"> </span>datasets$mnist$<span class="kw">read_data_sets</span>(<span class="st">"MNIST-data"</span>, <span class="dt">one_hot =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>Define model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>tf$<span class="kw">placeholder</span>(tf$float32, <span class="kw">shape</span>(<span class="ot">NULL</span>, 784L))
W &lt;-<span class="st"> </span>tf$<span class="kw">Variable</span>(tf$<span class="kw">zeros</span>(<span class="kw">shape</span>(784L, 10L)))
b &lt;-<span class="st"> </span>tf$<span class="kw">Variable</span>(tf$<span class="kw">zeros</span>(<span class="kw">shape</span>(10L)))

y &lt;-<span class="st"> </span>tf$nn$<span class="kw">softmax</span>(tf$<span class="kw">matmul</span>(x, W) +<span class="st"> </span>b)

<span class="co"># Define loss and optimizer</span>
y_ &lt;-<span class="st"> </span>tf$<span class="kw">placeholder</span>(tf$float32, <span class="kw">shape</span>(<span class="ot">NULL</span>, 10L))
cross_entropy &lt;-<span class="st"> </span>tf$<span class="kw">reduce_mean</span>(-tf$<span class="kw">reduce_sum</span>(y_ *<span class="st"> </span><span class="kw">log</span>(y), <span class="dt">reduction_indices=</span>1L))
train_step &lt;-<span class="st"> </span>tf$train$<span class="kw">GradientDescentOptimizer</span>(<span class="fl">0.5</span>)$<span class="kw">minimize</span>(cross_entropy)</code></pre></div>
<p>Create session and initialize variables</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sess &lt;-<span class="st"> </span>tf$<span class="kw">Session</span>()
sess$<span class="kw">run</span>(tf$<span class="kw">global_variables_initializer</span>())</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train</span>
for (i in <span class="dv">1</span>:<span class="dv">1000</span>) {
  batches &lt;-<span class="st"> </span>mnist$train$<span class="kw">next_batch</span>(100L)
  batch_xs &lt;-<span class="st"> </span>batches[[<span class="dv">1</span>]]
  batch_ys &lt;-<span class="st"> </span>batches[[<span class="dv">2</span>]]
  sess$<span class="kw">run</span>(train_step,
           <span class="dt">feed_dict =</span> <span class="kw">dict</span>(<span class="dt">x =</span> batch_xs, <span class="dt">y_ =</span> batch_ys))
}</code></pre></div>
<p>Test trained model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">correct_prediction &lt;-<span class="st"> </span>tf$<span class="kw">equal</span>(tf$<span class="kw">argmax</span>(y, 1L), tf$<span class="kw">argmax</span>(y_, 1L))
accuracy &lt;-<span class="st"> </span>tf$<span class="kw">reduce_mean</span>(tf$<span class="kw">cast</span>(correct_prediction, tf$float32))
sess$<span class="kw">run</span>(accuracy,
         <span class="dt">feed_dict =</span> <span class="kw">dict</span>(<span class="dt">x =</span> mnist$test$images, <span class="dt">y_ =</span> mnist$test$labels))</code></pre></div>
<pre><code>## [1] 0.9191</code></pre>
</div>
<div id="deep-mnist" class="section level2">
<h2 class="hasAnchor">
<a href="#deep-mnist" class="anchor"></a>Deep MNIST</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">weight_variable &lt;-<span class="st"> </span>function(shape) {
  initial &lt;-<span class="st"> </span>tf$<span class="kw">truncated_normal</span>(shape, <span class="dt">stddev=</span><span class="fl">0.1</span>)
  tf$<span class="kw">Variable</span>(initial)
}

bias_variable &lt;-<span class="st"> </span>function(shape) {
  initial &lt;-<span class="st"> </span>tf$<span class="kw">constant</span>(<span class="fl">0.1</span>, <span class="dt">shape=</span>shape)
  tf$<span class="kw">Variable</span>(initial)
}

conv2d &lt;-<span class="st"> </span>function(x, W) {
  tf$nn$<span class="kw">conv2d</span>(x, W, <span class="dt">strides=</span><span class="kw">c</span>(1L, 1L, 1L, 1L), <span class="dt">padding=</span><span class="st">'SAME'</span>)
}

max_pool_2x2 &lt;-<span class="st"> </span>function(x) {
  tf$nn$<span class="kw">max_pool</span>(
    x, 
    <span class="dt">ksize=</span><span class="kw">c</span>(1L, 2L, 2L, 1L),
    <span class="dt">strides=</span><span class="kw">c</span>(1L, 2L, 2L, 1L), 
    <span class="dt">padding=</span><span class="st">'SAME'</span>)
}

W_conv1 &lt;-<span class="st"> </span><span class="kw">weight_variable</span>(<span class="kw">shape</span>(5L, 5L, 1L, 32L))
b_conv1 &lt;-<span class="st"> </span><span class="kw">bias_variable</span>(<span class="kw">shape</span>(32L))

x_image &lt;-<span class="st"> </span>tf$<span class="kw">reshape</span>(x, <span class="kw">shape</span>(-1L, 28L, 28L, 1L))

h_conv1 &lt;-<span class="st"> </span>tf$nn$<span class="kw">relu</span>(<span class="kw">conv2d</span>(x_image, W_conv1) +<span class="st"> </span>b_conv1)
h_pool1 &lt;-<span class="st"> </span><span class="kw">max_pool_2x2</span>(h_conv1)

W_conv2 &lt;-<span class="st"> </span><span class="kw">weight_variable</span>(<span class="dt">shape =</span> <span class="kw">shape</span>(5L, 5L, 32L, 64L))
b_conv2 &lt;-<span class="st"> </span><span class="kw">bias_variable</span>(<span class="dt">shape =</span> <span class="kw">shape</span>(64L))

h_conv2 &lt;-<span class="st"> </span>tf$nn$<span class="kw">relu</span>(<span class="kw">conv2d</span>(h_pool1, W_conv2) +<span class="st"> </span>b_conv2)
h_pool2 &lt;-<span class="st"> </span><span class="kw">max_pool_2x2</span>(h_conv2)

W_fc1 &lt;-<span class="st"> </span><span class="kw">weight_variable</span>(<span class="kw">shape</span>(7L *<span class="st"> </span>7L *<span class="st"> </span>64L, 1024L))
b_fc1 &lt;-<span class="st"> </span><span class="kw">bias_variable</span>(<span class="kw">shape</span>(1024L))

h_pool2_flat &lt;-<span class="st"> </span>tf$<span class="kw">reshape</span>(h_pool2, <span class="kw">shape</span>(-1L, 7L *<span class="st"> </span>7L *<span class="st"> </span>64L))
h_fc1 &lt;-<span class="st"> </span>tf$nn$<span class="kw">relu</span>(tf$<span class="kw">matmul</span>(h_pool2_flat, W_fc1) +<span class="st"> </span>b_fc1)

keep_prob &lt;-<span class="st"> </span>tf$<span class="kw">placeholder</span>(tf$float32)
h_fc1_drop &lt;-<span class="st"> </span>tf$nn$<span class="kw">dropout</span>(h_fc1, keep_prob)

W_fc2 &lt;-<span class="st"> </span><span class="kw">weight_variable</span>(<span class="kw">shape</span>(1024L, 10L))
b_fc2 &lt;-<span class="st"> </span><span class="kw">bias_variable</span>(<span class="kw">shape</span>(10L))

y_conv &lt;-<span class="st"> </span>tf$nn$<span class="kw">softmax</span>(tf$<span class="kw">matmul</span>(h_fc1_drop, W_fc2) +<span class="st"> </span>b_fc2)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cross_entropy &lt;-<span class="st"> </span>tf$<span class="kw">reduce_mean</span>(-tf$<span class="kw">reduce_sum</span>(y_ *<span class="st"> </span>tf$<span class="kw">log</span>(y_conv), <span class="dt">reduction_indices=</span>1L))
train_step &lt;-<span class="st"> </span>tf$train$<span class="kw">AdamOptimizer</span>(<span class="fl">1e-4</span>)$<span class="kw">minimize</span>(cross_entropy)
correct_prediction &lt;-<span class="st"> </span>tf$<span class="kw">equal</span>(tf$<span class="kw">argmax</span>(y_conv, 1L), tf$<span class="kw">argmax</span>(y_, 1L))
accuracy &lt;-<span class="st"> </span>tf$<span class="kw">reduce_mean</span>(tf$<span class="kw">cast</span>(correct_prediction, tf$float32))
sess$<span class="kw">run</span>(tf$<span class="kw">global_variables_initializer</span>())

for (i in <span class="dv">1</span>:<span class="dv">20000</span>) {
  batch &lt;-<span class="st"> </span>mnist$train$<span class="kw">next_batch</span>(50L)
  if (i %%<span class="st"> </span><span class="dv">100</span> ==<span class="st"> </span><span class="dv">0</span>) {
    train_accuracy &lt;-<span class="st"> </span>accuracy$<span class="kw">eval</span>(<span class="dt">session=</span>sess, <span class="dt">feed_dict =</span> <span class="kw">dict</span>(
        <span class="dt">x =</span> batch[[<span class="dv">1</span>]], <span class="dt">y_ =</span> batch[[<span class="dv">2</span>]], <span class="dt">keep_prob =</span> <span class="fl">1.0</span>))
    <span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">"step %d, training accuracy %g</span><span class="ch">\n</span><span class="st">"</span>, i, train_accuracy))
  }
  train_step$<span class="kw">run</span>(<span class="dt">session=</span>sess, <span class="dt">feed_dict =</span> <span class="kw">dict</span>(
    <span class="dt">x =</span> batch[[<span class="dv">1</span>]], <span class="dt">y_ =</span> batch[[<span class="dv">2</span>]], <span class="dt">keep_prob =</span> <span class="fl">0.5</span>))
}</code></pre></div>
<pre><code>## step 100, training accuracy 0.94
## step 200, training accuracy 0.96
## step 300, training accuracy 0.84
## step 400, training accuracy 0.9
## step 500, training accuracy 0.96
## step 600, training accuracy 0.96
## step 700, training accuracy 0.98
## step 800, training accuracy 0.96
## step 900, training accuracy 0.96
## step 1000, training accuracy 0.92
## step 1100, training accuracy 0.98
## step 1200, training accuracy 0.94
## step 1300, training accuracy 0.96
## step 1400, training accuracy 0.98
## step 1500, training accuracy 0.92
## step 1600, training accuracy 1
## step 1700, training accuracy 0.98
## step 1800, training accuracy 0.96
## step 1900, training accuracy 0.98
## step 2000, training accuracy 0.96
## step 2100, training accuracy 1
## step 2200, training accuracy 0.98
## step 2300, training accuracy 0.98
## step 2400, training accuracy 0.94
## step 2500, training accuracy 0.98
## step 2600, training accuracy 0.96
## step 2700, training accuracy 1
## step 2800, training accuracy 0.96
## step 2900, training accuracy 0.98
## step 3000, training accuracy 1
## step 3100, training accuracy 0.98
## step 3200, training accuracy 1
## step 3300, training accuracy 0.98
## step 3400, training accuracy 0.98
## step 3500, training accuracy 1
## step 3600, training accuracy 1
## step 3700, training accuracy 1
## step 3800, training accuracy 1
## step 3900, training accuracy 0.98
## step 4000, training accuracy 1
## step 4100, training accuracy 0.98
## step 4200, training accuracy 1
## step 4300, training accuracy 1
## step 4400, training accuracy 0.98
## step 4500, training accuracy 1
## step 4600, training accuracy 0.96
## step 4700, training accuracy 1
## step 4800, training accuracy 1
## step 4900, training accuracy 1
## step 5000, training accuracy 0.98
## step 5100, training accuracy 0.98
## step 5200, training accuracy 1
## step 5300, training accuracy 1
## step 5400, training accuracy 1
## step 5500, training accuracy 0.98
## step 5600, training accuracy 1
## step 5700, training accuracy 1
## step 5800, training accuracy 1
## step 5900, training accuracy 0.98
## step 6000, training accuracy 0.98
## step 6100, training accuracy 1
## step 6200, training accuracy 1
## step 6300, training accuracy 0.98
## step 6400, training accuracy 1
## step 6500, training accuracy 0.98
## step 6600, training accuracy 0.98
## step 6700, training accuracy 1
## step 6800, training accuracy 1
## step 6900, training accuracy 1
## step 7000, training accuracy 1
## step 7100, training accuracy 1
## step 7200, training accuracy 1
## step 7300, training accuracy 1
## step 7400, training accuracy 0.98
## step 7500, training accuracy 1
## step 7600, training accuracy 1
## step 7700, training accuracy 1
## step 7800, training accuracy 1
## step 7900, training accuracy 1
## step 8000, training accuracy 1
## step 8100, training accuracy 1
## step 8200, training accuracy 1
## step 8300, training accuracy 0.98
## step 8400, training accuracy 0.96
## step 8500, training accuracy 1
## step 8600, training accuracy 1
## step 8700, training accuracy 1
## step 8800, training accuracy 0.98
## step 8900, training accuracy 0.98
## step 9000, training accuracy 1
## step 9100, training accuracy 1
## step 9200, training accuracy 1
## step 9300, training accuracy 1
## step 9400, training accuracy 1
## step 9500, training accuracy 0.98
## step 9600, training accuracy 1
## step 9700, training accuracy 1
## step 9800, training accuracy 0.98
## step 9900, training accuracy 0.98
## step 10000, training accuracy 1
## step 10100, training accuracy 0.98
## step 10200, training accuracy 0.98
## step 10300, training accuracy 0.98
## step 10400, training accuracy 1
## step 10500, training accuracy 0.98
## step 10600, training accuracy 0.98
## step 10700, training accuracy 1
## step 10800, training accuracy 0.98
## step 10900, training accuracy 0.98
## step 11000, training accuracy 1
## step 11100, training accuracy 1
## step 11200, training accuracy 1
## step 11300, training accuracy 1
## step 11400, training accuracy 1
## step 11500, training accuracy 1
## step 11600, training accuracy 1
## step 11700, training accuracy 1
## step 11800, training accuracy 1
## step 11900, training accuracy 1
## step 12000, training accuracy 1
## step 12100, training accuracy 0.98
## step 12200, training accuracy 1
## step 12300, training accuracy 1
## step 12400, training accuracy 1
## step 12500, training accuracy 1
## step 12600, training accuracy 1
## step 12700, training accuracy 1
## step 12800, training accuracy 1
## step 12900, training accuracy 1
## step 13000, training accuracy 1
## step 13100, training accuracy 1
## step 13200, training accuracy 1
## step 13300, training accuracy 1
## step 13400, training accuracy 1
## step 13500, training accuracy 1
## step 13600, training accuracy 1
## step 13700, training accuracy 1
## step 13800, training accuracy 1
## step 13900, training accuracy 1
## step 14000, training accuracy 0.98
## step 14100, training accuracy 1
## step 14200, training accuracy 1
## step 14300, training accuracy 1
## step 14400, training accuracy 1
## step 14500, training accuracy 1
## step 14600, training accuracy 1
## step 14700, training accuracy 0.98
## step 14800, training accuracy 1
## step 14900, training accuracy 1
## step 15000, training accuracy 1
## step 15100, training accuracy 1
## step 15200, training accuracy 0.98
## step 15300, training accuracy 1
## step 15400, training accuracy 1
## step 15500, training accuracy 1
## step 15600, training accuracy 1
## step 15700, training accuracy 1
## step 15800, training accuracy 1
## step 15900, training accuracy 1
## step 16000, training accuracy 1
## step 16100, training accuracy 1
## step 16200, training accuracy 1
## step 16300, training accuracy 1
## step 16400, training accuracy 1
## step 16500, training accuracy 1
## step 16600, training accuracy 1
## step 16700, training accuracy 1
## step 16800, training accuracy 1
## step 16900, training accuracy 1
## step 17000, training accuracy 1
## step 17100, training accuracy 1
## step 17200, training accuracy 1
## step 17300, training accuracy 1
## step 17400, training accuracy 1
## step 17500, training accuracy 1
## step 17600, training accuracy 1
## step 17700, training accuracy 1
## step 17800, training accuracy 1
## step 17900, training accuracy 1
## step 18000, training accuracy 1
## step 18100, training accuracy 1
## step 18200, training accuracy 1
## step 18300, training accuracy 1
## step 18400, training accuracy 1
## step 18500, training accuracy 1
## step 18600, training accuracy 1
## step 18700, training accuracy 1
## step 18800, training accuracy 1
## step 18900, training accuracy 1
## step 19000, training accuracy 1
## step 19100, training accuracy 1
## step 19200, training accuracy 1
## step 19300, training accuracy 1
## step 19400, training accuracy 1
## step 19500, training accuracy 1
## step 19600, training accuracy 1
## step 19700, training accuracy 1
## step 19800, training accuracy 1
## step 19900, training accuracy 1
## step 20000, training accuracy 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_accuracy &lt;-<span class="st"> </span>accuracy$<span class="kw">eval</span>(<span class="dt">session=</span>sess, <span class="dt">feed_dict =</span> <span class="kw">dict</span>(
     <span class="dt">x =</span> mnist$test$images, <span class="dt">y_ =</span> mnist$test$labels, <span class="dt">keep_prob =</span> <span class="fl">1.0</span>))
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">"test accuracy %g"</span>, train_accuracy))</code></pre></div>
<pre><code>## test accuracy 0.9932</code></pre>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#loading">Loading</a></li>
      <li><a href="#deep-mnist">Deep MNIST</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Chris Hua.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
